{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM2U_34C8Pem"
      },
      "source": [
        "# Building the simplest Graph\n",
        "\n",
        "We start with a graph with two nodes connected by one edge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0C8DrsE8Pen",
        "outputId": "b4562cfc-43d8-47eb-ad82-9ac85943ae99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from langgraph) (0.2.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.82)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (2.7.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (8.4.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2024.6.2)\n",
            "Collecting langchain_anthropic\n",
            "  Downloading langchain_anthropic-0.1.16-py3-none-any.whl (19 kB)\n",
            "Collecting anthropic<1,>=0.28.0 (from langchain_anthropic)\n",
            "  Downloading anthropic-0.30.0-py3-none-any.whl (863 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.5/863.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from langchain_anthropic) (0.7.1)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain_anthropic) (0.2.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain_anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic<1,>=0.28.0->langchain_anthropic) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from anthropic<1,>=0.28.0->langchain_anthropic)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jiter<1,>=0.4.0 (from anthropic<1,>=0.28.0->langchain_anthropic)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain_anthropic) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain_anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain_anthropic) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain_anthropic) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.10->langchain_anthropic) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.10->langchain_anthropic) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.10->langchain_anthropic) (0.1.82)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.10->langchain_anthropic) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.10->langchain_anthropic) (8.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.28.0->langchain_anthropic) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.28.0->langchain_anthropic) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.28.0->langchain_anthropic) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic<1,>=0.28.0->langchain_anthropic)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.28.0->langchain_anthropic)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.10->langchain_anthropic) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.10->langchain_anthropic) (3.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.10->langchain_anthropic) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<1,>=0.28.0->langchain_anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<1,>=0.28.0->langchain_anthropic) (2.18.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<1,>=0.28.0->langchain_anthropic) (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.28.0->langchain_anthropic) (3.15.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.28.0->langchain_anthropic) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.28.0->langchain_anthropic) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.10->langchain_anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.10->langchain_anthropic) (2.0.7)\n",
            "Installing collected packages: jiter, h11, httpcore, httpx, anthropic, langchain_anthropic\n",
            "Successfully installed anthropic-0.30.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 langchain_anthropic-0.1.16\n"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langgraph\n",
        "%pip install -U langchain_anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I92DpvRf8Peo"
      },
      "source": [
        "Nodes act like functions that can be called as needed. In our case Node 1 is our starting point and Node 2 is our finish point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA5LLWLr8Pep"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    return input_1 + \" Hi \"\n",
        "\n",
        "def function_2(input_2):\n",
        "    return input_2 + \"there\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxA9k5vk8Pep"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph\n",
        "\n",
        "# Define a Langchain graph\n",
        "workflow = Graph()\n",
        "\n",
        "workflow.add_node(\"node_1\", function_1)\n",
        "workflow.add_node(\"node_2\", function_2)\n",
        "\n",
        "workflow.add_edge('node_1', 'node_2')\n",
        "\n",
        "workflow.set_entry_point(\"node_1\")\n",
        "workflow.set_finish_point(\"node_2\")\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMU871qI8Pep",
        "outputId": "4ebc8910-7698-4d8f-93d9-79621090d647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Hi there'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "app.invoke(\"Hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NOCLv0S8Pep",
        "outputId": "a9841b10-cdfa-4201-b5f5-9e8de1c94ee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'node_1':\n",
            "---\n",
            "Hello Hi \n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'node_2':\n",
            "---\n",
            "Hello Hi there\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = 'Hello'\n",
        "for output in app.stream(input):\n",
        "    # stream() yields dictionaries with output keyed by node name\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuWBotIa8Peq"
      },
      "source": [
        "### As you can see, we can run the nodes as functions and return some values from them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMCwVkm78Peq"
      },
      "source": [
        "# Adding LLM Call\n",
        "\n",
        "Now, let's make the first node as an \"Agent\" that can call Open AI models. We can use langchain to make this call easy for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d8Na0lQ8Peq"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain langchain_openai\n",
        "%pip install -U google-generativeai\n",
        "%pip install langchain_google_genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NEOnkDF8Per"
      },
      "source": [
        "A usual call to ChatOpenAI model in LangChain is done as below:\n",
        "\n",
        "First set your API keys for OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq06_Jwb8Per",
        "outputId": "6088b251-821b-42c7-e765-6d9393821fd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWdA82FG8Per"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Now you can access your environment variables using os.environ\n",
        "os.environ['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTEjU3oJwMMy",
        "outputId": "cd440284-2930-4296-e107-220a4d354353"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GOOGLE_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzeNuhu28Per",
        "outputId": "23eb7f22-3749-44be-e161-2c33c7a33678"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Set the model as ChatOpenAI\n",
        "model = ChatOpenAI(temperature=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",\n",
        "      google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "      convert_system_message_to_human = True,\n",
        "      verbose = True,\n",
        ")\n",
        "#Call the model with a user message\n",
        "model.invoke('Hey there')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3SsMnz2wQ3G",
        "outputId": "ff882012-d04d-43fb-a19d-bcacacb3f581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hey there! What can I do for you today? \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-77bc77f0-2e17-4b8f-aa52-7e6b04d49927-0', usage_metadata={'input_tokens': 3, 'output_tokens': 12, 'total_tokens': 15})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGyafFCf8Pes"
      },
      "source": [
        "And if you just want to see the AI response, you can do the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7La2JVRw8Pes",
        "outputId": "30515442-cbd0-4cf7-b2af-b5c2bfb75ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey there! What can I do for you today? \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model.invoke('Hey there').content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_11_kyrt8Pes"
      },
      "source": [
        "Cool! Keeping that in mind, let's change the function 1 above so that we can send the user question to the model. Then we will send this response to function 2, which will add a short string and return to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31oCnRFb8Pes"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    response = model.invoke(input_1)\n",
        "    return response.content\n",
        "\n",
        "def function_2(input_2):\n",
        "    return \"Agent Says: \" + input_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EJz-tHW8Pes"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph\n",
        "\n",
        "# Define a Langchain graph\n",
        "workflow = Graph()\n",
        "\n",
        "#calling node 1 as agent\n",
        "workflow.add_node(\"agent\", function_1)\n",
        "workflow.add_node(\"node_2\", function_2)\n",
        "\n",
        "workflow.add_edge('agent', 'node_2')\n",
        "\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.set_finish_point(\"node_2\")\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMIZGMEN8Pes",
        "outputId": "1bd7b71a-bde5-4cb6-de46-c1b2fd9df241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agent Says: Hey there! What can I do for you today? \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "app.invoke(\"Hey there\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NHyAUR48Pet",
        "outputId": "d5fc933f-255f-41c1-85e8-e4a98bdbdd37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "Hey there! What can I do for you today? \n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'node_2':\n",
            "---\n",
            "Agent Says: Hey there! What can I do for you today? \n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = 'Hey there'\n",
        "for output in app.stream(input):\n",
        "    # stream() yields dictionaries with output keyed by node name\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qav2lHDi8Pet"
      },
      "source": [
        "# First functional Agent App - City Temperature\n",
        "\n",
        "### Step 1: Parse the city mentioned\n",
        "Let's extract the city that a user mentions in a query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3dZ2xLv8Pet"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
        "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
        "    response = model.invoke(complete_query)\n",
        "    return response.content\n",
        "\n",
        "def function_2(input_2):\n",
        "    return \"Agent Says: \" + input_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_MvGkDK8Pet"
      },
      "outputs": [],
      "source": [
        "# Define a Langchain graph\n",
        "workflow = Graph()\n",
        "\n",
        "#calling node 1 as agent\n",
        "workflow.add_node(\"agent\", function_1)\n",
        "workflow.add_node(\"node_2\", function_2)\n",
        "\n",
        "workflow.add_edge('agent', 'node_2')\n",
        "\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.set_finish_point(\"node_2\")\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THwaz6fI8Pet",
        "outputId": "a7de7a64-f415-4af2-d529-01ac6c578f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agent Says: Las Vegas \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "app.invoke(\"What's the temperature in Las Vegas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU08dWId8Peu"
      },
      "source": [
        "### Step 2: Adding a weather API call\n",
        "\n",
        "What if we want the function 2 to take the city name and give us the weather for that city.\n",
        "\n",
        "Well we know that Open Weather Map is [integrated](https://python.langchain.com/docs/integrations/tools/openweathermap) into LangChain\n",
        "\n",
        "We need to install pyown, create an API key on the website of Open Weather Map (which takes a few hours to activate) and then run the cells below to get weather of a given city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeZ5K_W88Peu",
        "outputId": "34894bdc-2bd8-4e36-e79e-787492720a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyowm in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyowm) (2.31.0)\n",
            "Requirement already satisfied: geojson<3,>=2.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyowm) (2.5.0)\n",
            "Requirement already satisfied: PySocks<2,>=1.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyowm) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.20.0->pyowm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.20.0->pyowm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2.20.0->pyowm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.20.0->pyowm) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyowm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5NoXS9s8Peu"
      },
      "outputs": [],
      "source": [
        "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
        "load_dotenv()\n",
        "os.environ[\"OPENWEATHERMAP_API_KEY\"] = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
        "\n",
        "weather = OpenWeatherMapAPIWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMJfLBlh8Peu",
        "outputId": "e1907b6e-217a-4fe5-cf3c-3ffe083c393f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In Las Vegas, the current weather is as follows:\n",
            "Detailed status: clear sky\n",
            "Wind speed: 3.6 m/s, direction: 90°\n",
            "Humidity: 33%\n",
            "Temperature: \n",
            "  - Current: 18.71°C\n",
            "  - High: 19.38°C\n",
            "  - Low: 18.16°C\n",
            "  - Feels like: 17.5°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 0%\n"
          ]
        }
      ],
      "source": [
        "weather_data = weather.run(\"Las Vegas\")\n",
        "print(weather_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6UA7Z758Pev"
      },
      "source": [
        "Now, let's integrate this into function 2 and call the function two as a \"tool\" or \"weather_agent\" instead of \"node_2\" in our workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mszcqGEa8Pev"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
        "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
        "    response = model.invoke(complete_query)\n",
        "    return response.content\n",
        "\n",
        "def function_2(input_2):\n",
        "    weather_data = weather.run(input_2)\n",
        "    return weather_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on4kgaDQ8Pev"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph\n",
        "\n",
        "workflow = Graph()\n",
        "\n",
        "#calling node 1 as agent\n",
        "workflow.add_node(\"agent\", function_1)\n",
        "workflow.add_node(\"tool\", function_2)\n",
        "\n",
        "workflow.add_edge('agent', 'tool')\n",
        "\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.set_finish_point(\"tool\")\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyAgR_2L8Pev",
        "outputId": "bc03a990-dce7-4304-e418-f397a61051ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 3.6 m/s, direction: 90°\\nHumidity: 33%\\nTemperature: \\n  - Current: 18.71°C\\n  - High: 19.38°C\\n  - Low: 18.16°C\\n  - Feels like: 17.5°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke(\"What's the temperature in Las Vegas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjIBX2Aw8Pev",
        "outputId": "70771899-4d0c-4b0d-cc6b-cb393b587a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "Las Vegas\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'tool':\n",
            "---\n",
            "In Las Vegas, the current weather is as follows:\n",
            "Detailed status: clear sky\n",
            "Wind speed: 3.6 m/s, direction: 90°\n",
            "Humidity: 33%\n",
            "Temperature: \n",
            "  - Current: 18.65°C\n",
            "  - High: 19.38°C\n",
            "  - Low: 18.16°C\n",
            "  - Feels like: 17.43°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 0%\n",
            "\n",
            "---\n",
            "\n",
            "Output from node '__end__':\n",
            "---\n",
            "In Las Vegas, the current weather is as follows:\n",
            "Detailed status: clear sky\n",
            "Wind speed: 3.6 m/s, direction: 90°\n",
            "Humidity: 33%\n",
            "Temperature: \n",
            "  - Current: 18.65°C\n",
            "  - High: 19.38°C\n",
            "  - Low: 18.16°C\n",
            "  - Feels like: 17.43°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 0%\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = \"What's the temperature in Las Vegas\"\n",
        "for output in app.stream(input):\n",
        "    # stream() yields dictionaries with output keyed by node name\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOOoNzsF8Pe2"
      },
      "source": [
        "Hopefully, that gives you a good understanding of how we built a LangGraph app and why we used different LC components."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}